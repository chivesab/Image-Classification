{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a6b0569-0bac-4937-af87-7f3d3e1a5d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in /usr/local/lib/python3.8/site-packages (0.1.10)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from opendatasets) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from opendatasets) (4.60.0)\n",
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/site-packages (from opendatasets) (1.5.12)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/site-packages (from kaggle->opendatasets) (2.8.1)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/site-packages (from kaggle->opendatasets) (5.0.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/site-packages (from kaggle->opendatasets) (2020.12.5)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/site-packages (from kaggle->opendatasets) (1.26.4)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/Cellar/protobuf/3.13.0/libexec/lib/python3.8/site-packages (from kaggle->opendatasets) (1.15.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from kaggle->opendatasets) (2.25.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from requests->kaggle->opendatasets) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->kaggle->opendatasets) (2.10)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.1.48-cp38-cp38-macosx_10_13_x86_64.whl (40.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 40.3 MB 36.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/site-packages (from opencv-python) (1.19.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.1.48\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opendatasets \n",
    "!pip3 install opencv-python\n",
    "import opendatasets as od\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import Image\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7db9a30-395c-4a00-8d22-6ce994406439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator():\n",
    "    def __init__(self, g1):\n",
    "#         super().__init__()\n",
    "        self.g1 = g1\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "class Generator():\n",
    "    def __init__(self, g1):\n",
    "#         super().__init__()\n",
    "        self.g1 = g1\n",
    "        self.network = nn.Sequential(\n",
    "        nn.ConvTranspose2d(g1.latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU(True),\n",
    "\n",
    "        nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(True),\n",
    "\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(True),\n",
    "\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(True),\n",
    "\n",
    "        nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "        nn.Tanh()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcea93d8-ea8f-486f-b958-d14644c7f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    def __init__(self, f1, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        self.f1 = f1\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield self.f1.to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68e9597f-f3c0-403b-8629-7e040e894205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        print(\"constructor\")\n",
    "        \n",
    "    def init_hyp(self):\n",
    "        self.image_size = 64\n",
    "        self.batch_size = 128\n",
    "        self.stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "        self.latent_size = 128\n",
    "        self.lr = 0.0002\n",
    "        self.epoch = 25\n",
    "        \n",
    "    def get_data(self):\n",
    "        dataset_url = 'https://www.kaggle.com/splcher/animefacedataset'\n",
    "        od.download(dataset_url)\n",
    "        \n",
    "    def show_data(self):\n",
    "        self.DATA_DIR = './data/animefacedataset'\n",
    "        print(os.listdir(self.DATA_DIR+'/images')[:10])\n",
    "    \n",
    "    def init_data(self):\n",
    "        self.init_hyp()\n",
    "        self.disc = Discriminator(self).network\n",
    "        self.genr = Generator(self).network\n",
    "        self.device = self.get_default_device()\n",
    "        self.to_device(self.disc, self.device)\n",
    "        self.to_device(self.genr, self.device)\n",
    "        self.fixed_latent = torch.randn(64, self.latent_size, 1, 1, device=self.device)\n",
    "        \n",
    "        self.train_ds = ImageFolder(self.DATA_DIR, transform=T.Compose([\n",
    "        T.Resize(self.image_size),\n",
    "        T.CenterCrop(self.image_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(*self.stats)]))\n",
    "\n",
    "        self.train_dl_cpu = DataLoader(self.train_ds, self.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        self.train_dl = DeviceDataLoader(self, self.train_dl_cpu, self.device)\n",
    "    \n",
    "    def denorm(self, img_tensors):\n",
    "        return img_tensors * self.stats[1][0] + self.stats[0][0]\n",
    "    \n",
    "    def show_images(self, images, nmax=64):\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(self.denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "\n",
    "    def show_batch(self, nmax=64):\n",
    "        for images, _ in self.train_dl_cpu:\n",
    "            self.show_images(images, nmax)\n",
    "            break\n",
    "    \n",
    "    def get_default_device(self):\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device('cuda')\n",
    "        else:\n",
    "            return torch.device('cpu')\n",
    "    \n",
    "    def to_device(self, data, device):\n",
    "        if isinstance(data, (list,tuple)):\n",
    "            return [self.to_device(x, device) for x in data]\n",
    "        return data.to(device, non_blocking=True)\n",
    "    \n",
    "    def train_discriminator(self, real_images, opt_d):\n",
    "        opt_d.zero_grad()\n",
    "\n",
    "        real_preds = self.disc(real_images)\n",
    "        real_targets = torch.ones(real_images.size(0), 1, device=self.device)\n",
    "        real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
    "        real_score = torch.mean(real_preds).item()\n",
    "\n",
    "        latent = torch.randn(self.batch_size, self.latent_size, 1, 1, device=self.device)\n",
    "        fake_images = self.genr(latent)\n",
    "\n",
    "        fake_targets = torch.zeros(fake_images.size(0), 1, device=self.device)\n",
    "        fake_preds = self.disc(fake_images)\n",
    "        fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
    "        fake_score = torch.mean(fake_preds).item()\n",
    "\n",
    "        loss = real_loss + fake_loss\n",
    "        loss.backward()\n",
    "        opt_d.step()\n",
    "        return loss.item(), real_score, fake_score\n",
    "    \n",
    "    def train_generator(self, opt_g):\n",
    "        # Clear generator gradients\n",
    "        opt_g.zero_grad()\n",
    "\n",
    "        # Generate fake images\n",
    "        latent = torch.randn(self.batch_size, self.latent_size, 1, 1, device=self.device)\n",
    "        fake_images = self.genr(latent)\n",
    "\n",
    "        # Try to fool the discriminator\n",
    "        preds = self.disc(fake_images)\n",
    "        targets = torch.ones(self.batch_size, 1, device=self.device)\n",
    "        loss = F.binary_cross_entropy(preds, targets)\n",
    "\n",
    "        # Update generator weights\n",
    "        loss.backward()\n",
    "        opt_g.step()\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "    def save_samples(self, index, latent_tensors, show=True):\n",
    "        sample_dir = './data/animefacedataset/generated'\n",
    "        os.makedirs(sample_dir, exist_ok=True)\n",
    "        \n",
    "        fake_images = self.genr(latent_tensors)\n",
    "        fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n",
    "        save_image(self.denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
    "        print('Saving', fake_fname)\n",
    "        if show:\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))\n",
    "    \n",
    "    def fit(self, start_idx=1):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Losses & scores\n",
    "        losses_g = []\n",
    "        losses_d = []\n",
    "        real_scores = []\n",
    "        fake_scores = []\n",
    "\n",
    "        # Create optimizers\n",
    "        opt_d = torch.optim.Adam(self.disc.parameters(), lr=self.lr, betas=(0.5, 0.999))\n",
    "        opt_g = torch.optim.Adam(self.genr.parameters(), lr=self.lr, betas=(0.5, 0.999))\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "            for real_images, _ in tqdm(self.train_dl, position=0, leave=True):\n",
    "                # Train discriminator\n",
    "                loss_d, real_score, fake_score = self.train_discriminator(real_images, opt_d)\n",
    "                # Train generator\n",
    "                loss_g = self.train_generator(opt_g)\n",
    "\n",
    "            # Record losses & scores\n",
    "            losses_g.append(loss_g)\n",
    "            losses_d.append(loss_d)\n",
    "            real_scores.append(real_score)\n",
    "            fake_scores.append(fake_score)\n",
    "\n",
    "            # Log losses & scores (last batch)\n",
    "            print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
    "                epoch+1, self.epoch, loss_g, loss_d, real_score, fake_score))\n",
    "\n",
    "            # Save generated images\n",
    "            self.save_samples(epoch+start_idx, self.fixed_latent, show=False)\n",
    "\n",
    "        return losses_g, losses_d, real_scores, fake_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08927056-1291-4eba-b17c-04879ae647f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructor\n"
     ]
    }
   ],
   "source": [
    "gan = GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "857b645f-9ed0-469e-8f6b-d2269e21b74f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/animefacedataset/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b792a9a0ed53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-d06b5de671f9>\u001b[0m in \u001b[0;36mshow_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/animefacedataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/animefacedataset/images'"
     ]
    }
   ],
   "source": [
    "gan.show_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "228c515b-3ba4-4258-b84b-3ae1dc5f5e64",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/animefacedataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-069a724399bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-d06b5de671f9>\u001b[0m in \u001b[0;36minit_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         self.train_ds = ImageFolder(self.DATA_DIR, transform=T.Compose([\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     ):\n\u001b[0;32m--> 253\u001b[0;31m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n\u001b[0m\u001b[1;32m    254\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    124\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m    125\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mNo\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \"\"\"\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/animefacedataset'"
     ]
    }
   ],
   "source": [
    "gan.init_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71719575-4f42-41ae-9029-57b6f9eee57d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GAN' object has no attribute 'train_dl_cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-eb26546ea4b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-d06b5de671f9>\u001b[0m in \u001b[0;36mshow_batch\u001b[0;34m(self, nmax)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl_cpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GAN' object has no attribute 'train_dl_cpu'"
     ]
    }
   ],
   "source": [
    "gan.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4573e67e-c391-4bff-aef5-f9c434519cb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GAN' object has no attribute 'train_dl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-4dc116d127c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-d06b5de671f9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_idx)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0;31m# Train discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mloss_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GAN' object has no attribute 'train_dl'"
     ]
    }
   ],
   "source": [
    "history = gan.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e186f7a2-682e-4061-a5a6-3571880ae64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71611d0-0423-48e6-a0a3-3b6ecff36826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9a94e-2b9e-42c1-83f9-7873cffae126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5e63f-dc11-4247-a89f-b4d2f482cc14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c93a5-a5bb-40e9-9a3f-c4414f5e23e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
